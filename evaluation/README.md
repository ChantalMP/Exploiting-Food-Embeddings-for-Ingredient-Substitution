## Summary
We evaluate the approachess either using human labels, or using our limited ground truth set.

## How it Works 
It compares the suggestions made from an approach (set 1) to our ground truth suggestions (set 2). The calculation of
precision and recall can also be seen as intersection over union over the two sets.

Precision: How many pairs in the ground truth were found  
Recall: How many pairs in the guesses were correct

## How to Run
If ground_truth_substitutes_dict.json was modified first run

    python -m evaluation.create_ground_truth_set

Then in evaluate_predictions_vs_ground_truth.py the correct substitutes_path should be specified.
Afterwards just run

    python -m evaluation.ground_truth_based_evaluation

To get the human evaluation results, run
    
    python -m evaluation.human_evaluation